{"meta":{"title":"AI for Science","subtitle":"","description":"my ai learning experience","author":"Tango","url":"http://zhangwenxiang.com","root":"/"},"pages":[{"title":"link","date":"2023-09-04T02:29:30.000Z","updated":"2023-09-04T02:30:19.096Z","comments":true,"path":"link/index.html","permalink":"http://zhangwenxiang.com/link/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-09-01T07:19:58.000Z","updated":"2023-09-01T07:20:42.492Z","comments":true,"path":"tags/index.html","permalink":"http://zhangwenxiang.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2023-09-01T07:21:00.000Z","updated":"2023-09-01T07:23:53.836Z","comments":true,"path":"categories/index.html","permalink":"http://zhangwenxiang.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2023-08-29T03:27:13.000Z","updated":"2023-09-01T07:25:40.988Z","comments":true,"path":"about/index.html","permalink":"http://zhangwenxiang.com/about/index.html","excerpt":"","text":"ABOUT ME"}],"posts":[{"title":"Yann LeCun：生成模型不适合处理视频，AI得在抽象空间中进行预测","slug":"Yann-LeCun：生成模型不适合处理视频，AI得在抽象空间中进行预测","date":"2024-01-23T06:57:11.000Z","updated":"2024-01-23T06:58:02.904Z","comments":true,"path":"2024/01/23/Yann-LeCun：生成模型不适合处理视频，AI得在抽象空间中进行预测/","link":"","permalink":"http://zhangwenxiang.com/2024/01/23/Yann-LeCun%EF%BC%9A%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8D%E9%80%82%E5%90%88%E5%A4%84%E7%90%86%E8%A7%86%E9%A2%91%EF%BC%8CAI%E5%BE%97%E5%9C%A8%E6%8A%BD%E8%B1%A1%E7%A9%BA%E9%97%B4%E4%B8%AD%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B/","excerpt":"","text":"“适合用来处理视频的模型并不是我们现在大范围应用的生成模型。而且新的模型应该学会在抽象的表征空间中预测，而不是在像素空间中。” Yann LeCun：大型语言模型或者一般的 NLP 系统，通常是用这种方法训练的。拿一段文本，然后故意删掉一些地方，接着用一个巨大的神经网络来重建文本，也就是预测那些被删掉的词，也就是通过删掉一些词来「破坏」文本。像 ChatGPT 和 Lama 这样的模型都是这样训练的。你只需删掉最后一个词就能训练它们。从技术上讲实际上更复杂，不过大体就是这么个意思，训练这个系统来重建输入中缺失的信息。 一个显而易见的想法就出现了，为什么我们不用图片来试试呢？拿一张图片，通过移除一小部分来损坏图像，然后训练大型神经网络来恢复它。但这并不奏效，或者说效果并不好。这方面已经有很多尝试，但都不太成功。对于视频也是一样。 我已经研究视频预测九年了。我一直在尝试预测，就是向系统展示一段视频，然后训练它预测接下来会发生什么。如果系统能做到这点，它可能就能理解世界的一些基本规律，就像文本系统试图预测下一个词一样。它得能理解句子的含义，但这也做不到。Yann LeCun：主要的问题在于，你的笔有一些特殊的摆放方式。当你把它丢下去的时候，它会沿着特定的轨迹下落。我们大多数人无法准确预测轨迹是什么，但可以预测物体会掉下来。婴儿大概要花九个月的时间，才能理解没有支撑的物体会掉下来。这种直觉上的物理知识，婴儿九个月就能学会，那我们怎样让机器也做到这一点呢？ Yann LeCun：其实目前还没有真正的解决办法。但目前最有希望的，至少是能用于图像识别的东西，说出来可能会让大家惊讶，并不是生成式的。 所以最有效的模型不是生成图像的，不是重建，也不是直接预测。它做的是在一个抽象的表征空间中进行预测，就像我无法准确预测你手中的笔会如何掉落一样。但我可以预测它将会掉落。在某种抽象的层面上，一支笔具体在哪里以及它的确切摆放方式和其他具体细节，我都可以做出预测。 所以，我们需要在抽象表征空间中预测，而不是具体的像素空间。这就是为什么像素空间的预测都失败了，因为它太复杂了。 Daphne Koller：但是这不仅仅是关于视频的问题。我认为婴儿学到的另一件事是因果的概念。他们通过对世界的干预，并观察发生的事情来学习的。而我们的 LLM 还没有做到这一点。它们完全是预测性引擎，只是在建立关联，没有真正理解因果关系。而理解因果关系，对于人类与物质世界的交互极为重要，尤其是在我们尝试将数字信息与实体世界联系起来的时候。这是当前模型中缺失的一项很重要的能力。这种能力在实际应用的模型中缺失，在计算机进行常识推理的能力中也是缺失的。当我们尝试将其应用于其他领域，无论是制造业、生物学还是任何与物理世界互动的领域时，这种能力也都是缺失的。 Yann LeCun：在具身系统中，它实际上是有效的。有些系统是建立在对世界的模型上的。比如，这里有一个表示在时间点 t 的世界状态的模型，这里是我可能会采取的行动。想一想，在时间点 t+1 世界的状态会是什么？这就是所谓的世界模型。如果你有了这种世界模型，你就可以规划一系列行动来达到一个特定目标。 目前，我们还没有任何基于这一原理的 AI 系统，除了非常简单的机器人系统。它们的学习速度并不快。因此，一旦我们能够扩展这种模型的规模，我们就能拥有能理解世界、理解物理世界的系统。它们可以规划，可以推理，可以理解因果关系。因为它们知道一个行动可能产生什么效果。它将以目标为导向。我们可以利用这种规划给它们设定目标，这就是人工智能系统的未来架构。在我看来，一旦我们搞清楚怎么实现这一切，就不会有人还愿意用目前的方式。 原视频地址：https://www.weforum.org/events/world-economic-forum-annual-meeting-2024/sessions/the-expanding-universe-of-generative-models/","categories":[],"tags":[]},{"title":"算法黑话","slug":"算法黑话","date":"2023-12-16T12:26:20.000Z","updated":"2023-12-16T12:35:56.211Z","comments":true,"path":"2023/12/16/算法黑话/","link":"","permalink":"http://zhangwenxiang.com/2023/12/16/%E7%AE%97%E6%B3%95%E9%BB%91%E8%AF%9D/","excerpt":"","text":"算法黑话祛魅 feature：一个数组 representation：还是一个数组 embedding：把输入映射成数组提高 泛化性：预测更准了 过拟合：训练过头了 attention：加权 adaptive：还是加权 few-shot learning：看了几个样本就学 zero-shot learning：一个没看就开始瞎蒙 self-supervised：自学 semi-supervised：教一点自学一点 unsupervised：没人教了，跟谁学？ end-to-end：一套操作，行云流水搞到底 multi-stage：发现不行，还得一步一步来 domain：我圈起来一堆样本，就管他叫一个domain transfer：我非得在这一堆样本上训练，用在另一堆样本上，就是不直接训练，就是玩～ adversarial：我加了一部分就是让loss增大 robust：很稳我不会让loss变大的（但也不容易变小了） state of the art（sota）：我（吹nb）第一 outperform：我虽然没第一，但是我比baseline强 baseline：(故意)选出来的方法，让我能够outperform empirically：我做实验了，不知道为啥work theoretically：我以为我知道为啥work，但没做实验，或者只做了个toy model multi-task：把几个loss加一起，完事 multi-domain：把几堆儿样本混一块训练，完事 multi-modality：把视频语音文字图像graph点云xxx混一块训练，完事 multi-domain multi-modal multi-media model：mua～mua～mua～mua……","categories":[],"tags":[]},{"title":"ArchWiki","slug":"ArchWiki","date":"2023-11-03T05:22:51.000Z","updated":"2023-11-03T05:24:43.031Z","comments":true,"path":"2023/11/03/ArchWiki/","link":"","permalink":"http://zhangwenxiang.com/2023/11/03/ArchWiki/","excerpt":"","text":"参考：佛教建筑 开发框架：NextJS 结构：","categories":[],"tags":[]},{"title":"NEXTjs","slug":"nextjs","date":"2023-11-03T03:52:27.000Z","updated":"2023-11-03T04:39:38.577Z","comments":true,"path":"2023/11/03/nextjs/","link":"","permalink":"http://zhangwenxiang.com/2023/11/03/nextjs/","excerpt":"","text":"NEXTjs官网","categories":[],"tags":[]},{"title":"建筑图像分类数据集","slug":"建筑图像分类数据集","date":"2023-10-17T11:59:51.000Z","updated":"2023-10-17T12:00:56.502Z","comments":true,"path":"2023/10/17/建筑图像分类数据集/","link":"","permalink":"http://zhangwenxiang.com/2023/10/17/%E5%BB%BA%E7%AD%91%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86/","excerpt":"","text":"ArchWorld作者：张文翔（SEU+XZMU）","categories":[],"tags":[{"name":"建筑图像","slug":"建筑图像","permalink":"http://zhangwenxiang.com/tags/%E5%BB%BA%E7%AD%91%E5%9B%BE%E5%83%8F/"},{"name":"分类","slug":"分类","permalink":"http://zhangwenxiang.com/tags/%E5%88%86%E7%B1%BB/"},{"name":"数据集","slug":"数据集","permalink":"http://zhangwenxiang.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"}]},{"title":"深度学习环境配置","slug":"深度学习环境配置","date":"2023-10-17T10:58:06.000Z","updated":"2023-10-17T12:11:38.294Z","comments":true,"path":"2023/10/17/深度学习环境配置/","link":"","permalink":"http://zhangwenxiang.com/2023/10/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Ubuntu22.04+CUDA12.2+cuDNN7.0+Anaconda+PytorchUbuntu22.04 CUDA12.2 cnDNN Anaconda 12bash Anaconda3-5.0.1-Linux-x86_64.shsouce ~/.bashrc PyTorch","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"http://zhangwenxiang.com/tags/ubuntu/"},{"name":"深度学习","slug":"深度学习","permalink":"http://zhangwenxiang.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Ubuntu安装问题汇总","slug":"Ubuntu安装问题汇总","date":"2023-10-16T12:53:15.000Z","updated":"2023-10-19T01:56:31.300Z","comments":true,"path":"2023/10/16/Ubuntu安装问题汇总/","link":"","permalink":"http://zhangwenxiang.com/2023/10/16/Ubuntu%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/","excerpt":"","text":"系统安装在官网下载Ubuntu镜像：Ubuntu 20.04.1 LTS (Focal Fossa)，选择Desktop Image版本，得到.iso的镜像文件。 黑屏无法进入安装界面123sudo gedit /etc/default/grubGRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash nomodeset“ sudo update-grub 选中引导界面，选中ubuntu，依据提示按e 1rw quiet splash nomodeset 锁定显卡自动更新关闭nouveau 1sudo vim /etc/modprobe.d/blacklist.conf 文件最后插入： 12blacklist nouveauoptions nouveau modeset=0 更新 1sudo update-initramfs -u GRUB1vim /etc/default/grub 找到 GRUB_HIDDEN_TIMEOUT&#x3D;0 这行，使用#注释掉，变成 #GRUB_HIDDEN_TIMEOUT&#x3D;0保存退出 1sudo update-grub 若不行，重装123sudo update-grub sudo grub-install /dev/sda sudo reboot #重启 &#x2F;etc&#x2F;default&#x2F;grub文件介绍GRUB_TIMEOUT&#x3D;10（默认是为10秒的）意思是等待10秒钟，设置为负数为一直等待操作启动的时候就会显示grub菜单了，如果10秒内不选择，则会自动进入系统进入grub快捷键shift 配置国内的源1cp /etc/apt/sources.list /etc/apt/sources.list.bak 1sudo vim /etc/apt/sources.list deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse 12sudo apt updatesudo apt upgrade 安装python &amp; pip12sudo apt install python3sudo apt install python3-pip 1sudo apt install ssh 安装Cuda 选择对应版本，下载[https://developer.nvidia.com/cuda-12-0-1-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=runfile_local] 按照提示安装，若提示“Existing package manager installation of the driver found. It is strongly ”，选择continue，去掉“Driver” 设置环境变量 1234567nano ~/.bashrc文件最后加入以下语句：export CUDA_HOME=/usr/local/cuda-12.0export LD_LIBRARY_PATH=$&#123;CUDA_HOME&#125;/lib64export PATH=$&#123;CUDA_HOME&#125;/bin:$&#123;PATH&#125;使文件生效source ~/.bashrc 验证安装 1nvcc -V 安装Anaconda123wget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.shbash Anaconda3-2023.03-Linux-x86_64.shnano ~/.bashrc 末尾添加： 12export PATH=&quot;~/anaconda3/bin&quot;:$PATHsource ~/anaconda3/bin/activate 1source ~/.bashrc 进入base环境 创建新环境123conda create -n pytorch python=3.10.9conda info --envsconda activate pytorch 安装pytorch1conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia 验证GPU1234import torchtorch.cuda.is_available()from torch.backends import cudnncudnn.is_available() 安装配置SSH12345678# 安装openssh-serversudo apt-get install openssh-server# 检查ssh服务是否启动，若输出sshd则启动成功sudo ps -e | grep ssh# 启动sshsudo service ssh start# 查看ssh状态service sshd status 安装包出现依赖包问题解决1234# 使用apt安装时出现依赖包缺失问题，改用aptitude安装sudo apt-get updatesudo apt-get install aptitudesudo aptitude install DEPENDS_NAME 查看显卡情况123456# 显示服务器上的GPU的情况nvidia-smi # 定时更新显示服务器上的GPU的情况nvidia-smi -l # nvidia-smi 设定刷新时间（秒）显示GPU使用情况watch -n 3 Reference：ubuntu 黑屏 进入不了图形界面 dev&#x2F;sda1: clean, 552599&#x2F;6111232 files, 7119295&#x2F;24414464 blocks 【保姆级教程】个人深度学习工作站配置指南","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangwenxiang.com/tags/linux/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://zhangwenxiang.com/tags/ubuntu/"}]},{"title":"深度学习数据集","slug":"深度学习数据集","date":"2023-10-11T03:35:27.000Z","updated":"2023-10-11T03:49:11.952Z","comments":true,"path":"2023/10/11/深度学习数据集/","link":"","permalink":"http://zhangwenxiang.com/2023/10/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E9%9B%86/","excerpt":"","text":"小目标检测、图像分类、图像识别宠物图像数据集数据集下载地址：http://m6z.cn/5TAgdC一个包含 37 个类别的宠物数据集，每个类别大约有 200 张图像。这些图像在比例、姿势和照明方面有很大的变化。所有图像都有相关的品种、头部 ROI 和像素级三元图分割的地面实况注释。 街景门牌号 (SVHN) 数据集数据集下载地址：http://m6z.cn/5ExMWbSVHN 是一个真实世界的图像数据集，用于开发机器学习和对象识别算法，对数据预处理和格式化的要求最低。它可以被视为与MNIST风格相似（例如，图像是经过裁剪的小数字），但包含一个数量级的更多标记数据（超过 600,000 个数字图像），并且来自一个更难、未解决的现实世界问题（识别自然场景图像中的数字和数字）。SVHN 是从谷歌街景图像中的门牌号获得的。 3D MNIST 数字识别图像数据数据集下载地址：http://m6z.cn/5SUfEd该数据集的目的是提供一种简单的方法来开始处理 3D 计算机视觉问题，例如 3D 形状识别。 文档影印和内容数据数据集下载地址：http://m6z.cn/6nF67SMediaTeam Oulu Document 数据集是一个文档扫描图像和文档内容数据集，包含 500篇 1975年之前的文档信息。 猫咪数据集数据集下载地址：http://m6z.cn/5TAgbwCAT 数据集包括超过 9,000 张猫图像。对于每张图像，猫的头部都有九个点的注释，眼睛两个，嘴巴一个，耳朵六个。 CBCL 街道场景数据数据集下载地址：http://m6z.cn/5TAgeAStreetScenes Challenge Framework 是用于对象检测的图像、注释、软件和性能测量的集合。每张图像都是从马萨诸塞州波士顿及其周边地区的 DSC-F717 相机拍摄的。然后用围绕 9 个对象类别的每个示例的多边形手动标记每个图像，包括 [汽车、行人、自行车、建筑物、树木、天空、道路、人行道和商店]。这些图像的标记是在仔细检查下完成的，以确保对象总是以相同的方式标记，关于遮挡和其他常见的图像变换。 小目标检测数据集数据集下载地址：http://m6z.cn/616t6R从Internet（例如YouTube或Google）上的图像&#x2F;视频收集的四个小物体数据集，包括4种类型的图像，可用于小物体目标检测的研究。数据集包含四类：fly：飞行数据集，包含600个视频帧，平均每帧86±39个物体（648×72 @ 30 fps）。32张图像用于训练（1：6：187），50张图像用于测试（301：6：600）。honeybee：蜜蜂数据集，包含118张图像，每张图像平均有28±6个蜜蜂（640×480）。数据集被平均分配用于训练和测试集。仅前32张图像用于训练。seagull：海鸥数据集，包含三个高分辨率图像（624×964），每个图像平均有866±107个海鸥。第一张图片用于训练，其余图片用于测试。fish：鱼数据集，包含387帧视频数据，平均每帧56±9条鱼（300×410 @ 30 fps）。32张图像进行训练（1：3：94），65张图像进行测试（193：3：387）。 斯坦福狗狗数据集数据集下载地址：http://m6z.cn/6nF6kM斯坦福狗数据集包含来自世界各地的 120 种狗的图像。该数据集是使用 ImageNet 中的图像和注释构建的，用于细粒度图像分类任务。该数据集的内容：类别数：120图片数量：20,580注释：类标签、边界框 Zero-Shot Learing问题数据集提供几个最常用的Zero-Shot Learning的数据集，均为GoogleNet提取的图片特征，引用相应数据时，请注意对应作者的引用说明。 AwA:http://pan.baidu.com/s/1nvPzsXb CUB:http://pan.baidu.com/s/1nv3KCYH aPaY:http://pan.baidu.com/s/1hseSzVe SUN:http://pan.baidu.com/s/1gfAc33X ImageNet2:http://pan.baidu.com/s/1pLfZYQ","categories":[{"name":"dataset","slug":"dataset","permalink":"http://zhangwenxiang.com/categories/dataset/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://zhangwenxiang.com/tags/deeplearning/"},{"name":"dataset","slug":"dataset","permalink":"http://zhangwenxiang.com/tags/dataset/"}]},{"title":"What is a Convolutional Neural Network?","slug":"What-is-a-Convolutional-Neural-Network","date":"2023-10-11T01:58:05.000Z","updated":"2023-10-11T03:44:40.538Z","comments":true,"path":"2023/10/11/What-is-a-Convolutional-Neural-Network/","link":"","permalink":"http://zhangwenxiang.com/2023/10/11/What-is-a-Convolutional-Neural-Network/","excerpt":"","text":"What is a Convolutional Neural Network?(https://poloclub.github.io/cnn-explainer/#article-input) 动态展示CNN运行过程。","categories":[{"name":"CNN","slug":"CNN","permalink":"http://zhangwenxiang.com/categories/CNN/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://zhangwenxiang.com/tags/deeplearning/"},{"name":"cnn","slug":"cnn","permalink":"http://zhangwenxiang.com/tags/cnn/"}]},{"title":"论文阅读记录","slug":"论文阅读记录","date":"2023-09-26T01:42:40.000Z","updated":"2023-09-26T01:44:13.723Z","comments":true,"path":"2023/09/26/论文阅读记录/","link":"","permalink":"http://zhangwenxiang.com/2023/09/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/","excerpt":"","text":"本文记录如何阅读文献，试图找到更好的阅读和记录方法。 1.找到目标文章","categories":[],"tags":[]},{"title":"CV 相关问题","slug":"CV相关问题","date":"2023-09-18T09:31:43.000Z","updated":"2023-09-18T09:32:42.757Z","comments":true,"path":"2023/09/18/CV相关问题/","link":"","permalink":"http://zhangwenxiang.com/2023/09/18/CV%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"CNN网络结构的发展","categories":[],"tags":[{"name":"cv","slug":"cv","permalink":"http://zhangwenxiang.com/tags/cv/"}]},{"title":"Transformer 相关问题","slug":"Transformer相关问题","date":"2023-09-18T09:13:14.000Z","updated":"2023-09-19T09:33:01.696Z","comments":true,"path":"2023/09/18/Transformer相关问题/","link":"","permalink":"http://zhangwenxiang.com/2023/09/18/Transformer%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"Transformer为什么 dot-product attention 需要被 scaled？ Attention&#x2F;Self-Attention注意力机制到底在做什么，Q&#x2F;K&#x2F;V怎么来的？一文读懂Attention注意力机制 超详细图解Self-Attention 狗都能看懂的Self-Attention讲解 详解Transformer中Self-Attention以及Multi-Head Attention Vision Transformer狗都能看懂的Vision Transformer的讲解和代码实现 Vision Transformer详解 VIT(vision transformer)模型介绍+pytorch代码炸裂解析 Vision Transformer (ViT)模型与代码实现（PyTorch） 【超详细】初学者包会的Vision Transformer（ViT）的PyTorch实现代码学习 VIDEOVIT(vision transformer)模型介绍+pytorch代码炸裂解析Transformer中Self-Attention以及Multi-Head Attention详解Attention、Transformer公式推导和矩阵变化 CODEVision Transformer and MLP-Mixer ArchitecturesWZMIAOMIAO&#x2F;deep-learning-for-image-processing 延伸阅读近两年有哪些ViT(Vision Transformer)的改进算法？基于ViT的精细化分类算法介绍","categories":[{"name":"questions","slug":"questions","permalink":"http://zhangwenxiang.com/categories/questions/"},{"name":"toturial","slug":"questions/toturial","permalink":"http://zhangwenxiang.com/categories/questions/toturial/"}],"tags":[{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"vit","slug":"vit","permalink":"http://zhangwenxiang.com/tags/vit/"}]},{"title":"Personalized Federated Learning 个性化联邦学习","slug":"Personalized-Federated-Learning-个性化联邦学习","date":"2023-09-15T07:57:53.000Z","updated":"2023-09-18T09:13:43.525Z","comments":true,"path":"2023/09/15/Personalized-Federated-Learning-个性化联邦学习/","link":"","permalink":"http://zhangwenxiang.com/2023/09/15/Personalized-Federated-Learning-%E4%B8%AA%E6%80%A7%E5%8C%96%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Ref：个性化联邦学习 Towards Personalized Federated Learning","categories":[],"tags":[]},{"title":"timm","slug":"timm","date":"2023-09-15T02:54:39.000Z","updated":"2023-09-23T05:56:05.857Z","comments":true,"path":"2023/09/15/timm/","link":"","permalink":"http://zhangwenxiang.com/2023/09/15/timm/","excerpt":"","text":"timm timm install方法1. pip install timm方法2.","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://zhangwenxiang.com/categories/tutorial/"}],"tags":[{"name":"timm","slug":"timm","permalink":"http://zhangwenxiang.com/tags/timm/"},{"name":"pytorch","slug":"pytorch","permalink":"http://zhangwenxiang.com/tags/pytorch/"}]},{"title":"Vision Transformer(ViT)","slug":"Vision-Transformer-ViT","date":"2023-09-08T02:48:00.000Z","updated":"2023-09-26T01:44:49.877Z","comments":true,"path":"2023/09/08/Vision-Transformer-ViT/","link":"","permalink":"http://zhangwenxiang.com/2023/09/08/Vision-Transformer-ViT/","excerpt":"","text":"AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE[原文]（https://arxiv.org/abs/2010.11929）中文 Vision Transformer(ViT)Paper:An Image is Worth 16x16 Words: Transformers for Image Recognition at ScaleDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … Houlsby, N. (2020). arXiv: Computer Vision and Pattern Recognition ABSTRACTWhile the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. Notes:什么是Transformer位置编码？ Video:Vision Transformer (ViT) 用于图片分类 使用pytorch搭建Vision Transformer(vit)模型 Reference:[1]DOSOVITSKIY A, BEYER L, KOLESNIKOV A, et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale[J]. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition, 2020.","categories":[{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/categories/article/"}],"tags":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"},{"name":"cv","slug":"cv","permalink":"http://zhangwenxiang.com/tags/cv/"},{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/tags/article/"},{"name":"ai","slug":"ai","permalink":"http://zhangwenxiang.com/tags/ai/"},{"name":"model","slug":"model","permalink":"http://zhangwenxiang.com/tags/model/"},{"name":"vit","slug":"vit","permalink":"http://zhangwenxiang.com/tags/vit/"}]},{"title":"向量(vector)和张量(tensor)","slug":"向量-vector-和张量-tensor","date":"2023-09-07T03:33:57.000Z","updated":"2023-09-19T09:33:36.201Z","comments":true,"path":"2023/09/07/向量-vector-和张量-tensor/","link":"","permalink":"http://zhangwenxiang.com/2023/09/07/%E5%90%91%E9%87%8F-vector-%E5%92%8C%E5%BC%A0%E9%87%8F-tensor/","excerpt":"","text":"张量是多维数组，目的是把向量、矩阵推向更高的维度。 点——标量（scalar）线——向量（vector）面——矩阵（matrix）体——张量（tensor） 向量（vector）张量（tensor）url：PyTorch中张量的使用：http://t.csdn.cn/UdkHE PyTorchshape: 张量的形状，即各维度的大小。dtype: 张量的数据类型，例如float32、int64等。device: 张量存放的设备，例如cpu或cuda。Ref:张量（tensor）图解Vit 1：Vision Transformer——图像与Transformer基础","categories":[],"tags":[]},{"title":"Attention Is All You Need","slug":"Attention-Is-All-You-Need","date":"2023-09-06T07:38:44.000Z","updated":"2023-09-08T03:32:32.462Z","comments":true,"path":"2023/09/06/Attention-Is-All-You-Need/","link":"","permalink":"http://zhangwenxiang.com/2023/09/06/Attention-Is-All-You-Need/","excerpt":"","text":"ATTENTION IS ALL YOU NEEDPaper:Notes:Video:","categories":[{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/categories/article/"}],"tags":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"},{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/tags/article/"},{"name":"ai","slug":"ai","permalink":"http://zhangwenxiang.com/tags/ai/"},{"name":"nlp","slug":"nlp","permalink":"http://zhangwenxiang.com/tags/nlp/"},{"name":"model","slug":"model","permalink":"http://zhangwenxiang.com/tags/model/"}]},{"title":"NVIDIA驱动问题","slug":"NVIDIA驱动问题","date":"2023-09-06T01:59:02.000Z","updated":"2023-09-06T02:08:22.298Z","comments":true,"path":"2023/09/06/NVIDIA驱动问题/","link":"","permalink":"http://zhangwenxiang.com/2023/09/06/NVIDIA%E9%A9%B1%E5%8A%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题1：Linux服务器深度学习代码无法用GPU执行；问题2：执行命令 nvidia-smi，提示：Failed to initialize NVML: Driver&#x2F;library version mismatch原因：系统自动更新驱动分析：1.查看nvidia相关安装包信息，确认一下版本 1sudo dpkg --list | grep nvidia-* 2.查看nvidia内核版本 1cat /proc/driver/nvidia/version 3.查看安装包安装或更新情况 1cat /var/log/dpkg.log | grep nvidia 核对发现服务驱动自动更新了，所以导致显卡驱动用不了，执行不力nvidia-smi 解决办法：禁用自动更新1sudo vim /etc/apt/apt.conf.d/50unattended-upgrades 注释掉以下两行： &#x2F;&#x2F;“${distro_id}:${distro_codename}”; &#x2F;&#x2F;“${distro_id}:${distro_codename}-security”; 重启系统1sudo reboot 再次执行： 1nvidia-smi 正常： 123456789101112131415161718192021(base) dino@jarvis:~$ nvidia-smi Wed Sep 6 01:43:33 2023 +-----------------------------------------------------------------------------+| NVIDIA-SMI 525.125.06 Driver Version: 525.125.06 CUDA Version: 12.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A || 0% 57C P0 107W / 350W | 0MiB / 12288MiB | 0% Default || | | N/A |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ Ref: https://zhuanlan.zhihu.com/p/453955370","categories":[{"name":"tech","slug":"tech","permalink":"http://zhangwenxiang.com/categories/tech/"},{"name":"linux","slug":"tech/linux","permalink":"http://zhangwenxiang.com/categories/tech/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangwenxiang.com/tags/linux/"},{"name":"nvidia","slug":"nvidia","permalink":"http://zhangwenxiang.com/tags/nvidia/"},{"name":"驱动","slug":"驱动","permalink":"http://zhangwenxiang.com/tags/%E9%A9%B1%E5%8A%A8/"},{"name":"更新","slug":"更新","permalink":"http://zhangwenxiang.com/tags/%E6%9B%B4%E6%96%B0/"}]},{"title":"Linux连接SEU校园网","slug":"Linux连接SEU校园网","date":"2023-09-06T01:47:52.000Z","updated":"2023-09-06T01:59:56.659Z","comments":true,"path":"2023/09/06/Linux连接SEU校园网/","link":"","permalink":"http://zhangwenxiang.com/2023/09/06/Linux%E8%BF%9E%E6%8E%A5SEU%E6%A0%A1%E5%9B%AD%E7%BD%91/","excerpt":"","text":"SEU校园网登录页面：w.seu.edu.cn由于Linux更新显卡驱动后需要重启，导致IP重新分配。连接校园网后需要执行Web登录页面操作，Linux命令行界面如何登录，采用curl指令操作。 Linux下执行如下命令：1$ curl &quot;w.seu.edu.cn&quot; -d &quot;a=find_mac&amp;DDDDD=学号&amp;upass=密码&amp;0MKKey=&quot;","categories":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"},{"name":"tech","slug":"phd/tech","permalink":"http://zhangwenxiang.com/categories/phd/tech/"},{"name":"linux","slug":"phd/tech/linux","permalink":"http://zhangwenxiang.com/categories/phd/tech/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangwenxiang.com/tags/linux/"},{"name":"seu","slug":"seu","permalink":"http://zhangwenxiang.com/tags/seu/"},{"name":"东南大学","slug":"东南大学","permalink":"http://zhangwenxiang.com/tags/%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6/"},{"name":"校园网","slug":"校园网","permalink":"http://zhangwenxiang.com/tags/%E6%A0%A1%E5%9B%AD%E7%BD%91/"}]},{"title":"todo list 9.4","slug":"todo-list-9-4","date":"2023-09-04T13:25:28.000Z","updated":"2023-09-04T13:36:46.406Z","comments":true,"path":"2023/09/04/todo-list-9-4/","link":"","permalink":"http://zhangwenxiang.com/2023/09/04/todo-list-9-4/","excerpt":"","text":"近两日睡眠不太好，入睡太晚。上午到实验室把宣传片完成之后便进入东张西望的状态。应该是目标没有那么具体导致的注意力不集中，因为半天或者一天的任务无法达到明确的预期，所以迟迟不能进入状态。明早把手头之前的总结内容看完，列出具体的任务清单。将任务分解后逐步完成。 接下来的具体任务：Classification by ViT","categories":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"}],"tags":[{"name":"todo-list","slug":"todo-list","permalink":"http://zhangwenxiang.com/tags/todo-list/"}]},{"title":"宣传片","slug":"宣传片","date":"2023-09-04T02:09:30.000Z","updated":"2023-09-04T02:12:38.631Z","comments":true,"path":"2023/09/04/宣传片/","link":"","permalink":"http://zhangwenxiang.com/2023/09/04/%E5%AE%A3%E4%BC%A0%E7%89%87/","excerpt":"","text":"西藏自治区网络安全和信息化展览从收到素材大概一天半时间，我的周末啊！","categories":[{"name":"work","slug":"work","permalink":"http://zhangwenxiang.com/categories/work/"}],"tags":[{"name":"tibet","slug":"tibet","permalink":"http://zhangwenxiang.com/tags/tibet/"},{"name":"宣传片","slug":"宣传片","permalink":"http://zhangwenxiang.com/tags/%E5%AE%A3%E4%BC%A0%E7%89%87/"}]},{"title":"A New Start","slug":"A-New-Start","date":"2023-09-01T07:01:24.000Z","updated":"2023-09-01T07:40:29.277Z","comments":true,"path":"2023/09/01/A-New-Start/","link":"","permalink":"http://zhangwenxiang.com/2023/09/01/A-New-Start/","excerpt":"","text":"重新开始生涯记录，在SEU博士第三年伊始。 问题学习阅读不连续外界干扰很多，有工作的有生活的也有杂七杂八的事情。 每日总结每天结束时记录工作情况，思考并总结。","categories":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"}],"tags":[{"name":"diary","slug":"diary","permalink":"http://zhangwenxiang.com/tags/diary/"},{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"},{"name":"notes","slug":"notes","permalink":"http://zhangwenxiang.com/tags/notes/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-08-29T02:55:29.224Z","updated":"2023-08-29T02:55:29.224Z","comments":true,"path":"2023/08/29/hello-world/","link":"","permalink":"http://zhangwenxiang.com/2023/08/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"dataset","slug":"dataset","permalink":"http://zhangwenxiang.com/categories/dataset/"},{"name":"CNN","slug":"CNN","permalink":"http://zhangwenxiang.com/categories/CNN/"},{"name":"questions","slug":"questions","permalink":"http://zhangwenxiang.com/categories/questions/"},{"name":"toturial","slug":"questions/toturial","permalink":"http://zhangwenxiang.com/categories/questions/toturial/"},{"name":"tutorial","slug":"tutorial","permalink":"http://zhangwenxiang.com/categories/tutorial/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/categories/article/"},{"name":"tech","slug":"tech","permalink":"http://zhangwenxiang.com/categories/tech/"},{"name":"linux","slug":"tech/linux","permalink":"http://zhangwenxiang.com/categories/tech/linux/"},{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"},{"name":"tech","slug":"phd/tech","permalink":"http://zhangwenxiang.com/categories/phd/tech/"},{"name":"linux","slug":"phd/tech/linux","permalink":"http://zhangwenxiang.com/categories/phd/tech/linux/"},{"name":"work","slug":"work","permalink":"http://zhangwenxiang.com/categories/work/"}],"tags":[{"name":"建筑图像","slug":"建筑图像","permalink":"http://zhangwenxiang.com/tags/%E5%BB%BA%E7%AD%91%E5%9B%BE%E5%83%8F/"},{"name":"分类","slug":"分类","permalink":"http://zhangwenxiang.com/tags/%E5%88%86%E7%B1%BB/"},{"name":"数据集","slug":"数据集","permalink":"http://zhangwenxiang.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://zhangwenxiang.com/tags/ubuntu/"},{"name":"深度学习","slug":"深度学习","permalink":"http://zhangwenxiang.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"linux","slug":"linux","permalink":"http://zhangwenxiang.com/tags/linux/"},{"name":"deeplearning","slug":"deeplearning","permalink":"http://zhangwenxiang.com/tags/deeplearning/"},{"name":"dataset","slug":"dataset","permalink":"http://zhangwenxiang.com/tags/dataset/"},{"name":"cnn","slug":"cnn","permalink":"http://zhangwenxiang.com/tags/cnn/"},{"name":"cv","slug":"cv","permalink":"http://zhangwenxiang.com/tags/cv/"},{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"vit","slug":"vit","permalink":"http://zhangwenxiang.com/tags/vit/"},{"name":"timm","slug":"timm","permalink":"http://zhangwenxiang.com/tags/timm/"},{"name":"pytorch","slug":"pytorch","permalink":"http://zhangwenxiang.com/tags/pytorch/"},{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/tags/article/"},{"name":"ai","slug":"ai","permalink":"http://zhangwenxiang.com/tags/ai/"},{"name":"model","slug":"model","permalink":"http://zhangwenxiang.com/tags/model/"},{"name":"nlp","slug":"nlp","permalink":"http://zhangwenxiang.com/tags/nlp/"},{"name":"nvidia","slug":"nvidia","permalink":"http://zhangwenxiang.com/tags/nvidia/"},{"name":"驱动","slug":"驱动","permalink":"http://zhangwenxiang.com/tags/%E9%A9%B1%E5%8A%A8/"},{"name":"更新","slug":"更新","permalink":"http://zhangwenxiang.com/tags/%E6%9B%B4%E6%96%B0/"},{"name":"seu","slug":"seu","permalink":"http://zhangwenxiang.com/tags/seu/"},{"name":"东南大学","slug":"东南大学","permalink":"http://zhangwenxiang.com/tags/%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6/"},{"name":"校园网","slug":"校园网","permalink":"http://zhangwenxiang.com/tags/%E6%A0%A1%E5%9B%AD%E7%BD%91/"},{"name":"todo-list","slug":"todo-list","permalink":"http://zhangwenxiang.com/tags/todo-list/"},{"name":"tibet","slug":"tibet","permalink":"http://zhangwenxiang.com/tags/tibet/"},{"name":"宣传片","slug":"宣传片","permalink":"http://zhangwenxiang.com/tags/%E5%AE%A3%E4%BC%A0%E7%89%87/"},{"name":"diary","slug":"diary","permalink":"http://zhangwenxiang.com/tags/diary/"},{"name":"notes","slug":"notes","permalink":"http://zhangwenxiang.com/tags/notes/"}]}