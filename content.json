{"meta":{"title":"AI for Science","subtitle":"","description":"my ai learning experience","author":"Dino","url":"http://zhangwenxiang.com","root":"/"},"pages":[{"title":"about","date":"2023-08-29T03:27:13.000Z","updated":"2023-09-01T07:25:40.988Z","comments":true,"path":"about/index.html","permalink":"http://zhangwenxiang.com/about/index.html","excerpt":"","text":"ABOUT ME"},{"title":"categories","date":"2023-09-01T07:21:00.000Z","updated":"2023-09-01T07:23:53.836Z","comments":true,"path":"categories/index.html","permalink":"http://zhangwenxiang.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-09-01T07:19:58.000Z","updated":"2023-09-01T07:20:42.492Z","comments":true,"path":"tags/index.html","permalink":"http://zhangwenxiang.com/tags/index.html","excerpt":"","text":""},{"title":"link","date":"2023-09-04T02:29:30.000Z","updated":"2023-09-04T02:30:19.096Z","comments":true,"path":"link/index.html","permalink":"http://zhangwenxiang.com/link/index.html","excerpt":"","text":""}],"posts":[{"title":"CV 相关问题","slug":"CV相关问题","date":"2023-09-18T09:31:43.000Z","updated":"2023-09-18T09:32:42.757Z","comments":true,"path":"2023/09/18/CV相关问题/","link":"","permalink":"http://zhangwenxiang.com/2023/09/18/CV%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"CNN网络结构的发展","categories":[],"tags":[{"name":"cv","slug":"cv","permalink":"http://zhangwenxiang.com/tags/cv/"}]},{"title":"Transformer 相关问题","slug":"Transformer相关问题","date":"2023-09-18T09:13:14.000Z","updated":"2023-09-19T09:33:01.696Z","comments":true,"path":"2023/09/18/Transformer相关问题/","link":"","permalink":"http://zhangwenxiang.com/2023/09/18/Transformer%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"Transformer为什么 dot-product attention 需要被 scaled？ Attention&#x2F;Self-Attention注意力机制到底在做什么，Q&#x2F;K&#x2F;V怎么来的？一文读懂Attention注意力机制 超详细图解Self-Attention 狗都能看懂的Self-Attention讲解 详解Transformer中Self-Attention以及Multi-Head Attention Vision Transformer狗都能看懂的Vision Transformer的讲解和代码实现 Vision Transformer详解 VIT(vision transformer)模型介绍+pytorch代码炸裂解析 Vision Transformer (ViT)模型与代码实现（PyTorch） 【超详细】初学者包会的Vision Transformer（ViT）的PyTorch实现代码学习 VIDEOVIT(vision transformer)模型介绍+pytorch代码炸裂解析Transformer中Self-Attention以及Multi-Head Attention详解Attention、Transformer公式推导和矩阵变化 CODEVision Transformer and MLP-Mixer ArchitecturesWZMIAOMIAO&#x2F;deep-learning-for-image-processing 延伸阅读近两年有哪些ViT(Vision Transformer)的改进算法？基于ViT的精细化分类算法介绍","categories":[{"name":"questions","slug":"questions","permalink":"http://zhangwenxiang.com/categories/questions/"},{"name":"toturial","slug":"questions/toturial","permalink":"http://zhangwenxiang.com/categories/questions/toturial/"}],"tags":[{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"vit","slug":"vit","permalink":"http://zhangwenxiang.com/tags/vit/"}]},{"title":"Personalized Federated Learning 个性化联邦学习","slug":"Personalized-Federated-Learning-个性化联邦学习","date":"2023-09-15T07:57:53.000Z","updated":"2023-09-18T09:13:43.525Z","comments":true,"path":"2023/09/15/Personalized-Federated-Learning-个性化联邦学习/","link":"","permalink":"http://zhangwenxiang.com/2023/09/15/Personalized-Federated-Learning-%E4%B8%AA%E6%80%A7%E5%8C%96%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Ref：个性化联邦学习 Towards Personalized Federated Learning","categories":[],"tags":[]},{"title":"timm","slug":"timm","date":"2023-09-15T02:54:39.000Z","updated":"2023-09-15T02:56:30.922Z","comments":true,"path":"2023/09/15/timm/","link":"","permalink":"http://zhangwenxiang.com/2023/09/15/timm/","excerpt":"","text":"timm timm install方法1. pip install timm方法2.","categories":[{"name":"tutorial","slug":"tutorial","permalink":"http://zhangwenxiang.com/categories/tutorial/"}],"tags":[{"name":"timm","slug":"timm","permalink":"http://zhangwenxiang.com/tags/timm/"},{"name":"pytorch","slug":"pytorch","permalink":"http://zhangwenxiang.com/tags/pytorch/"}]},{"title":"Vision Transformer(ViT)","slug":"Vision-Transformer-ViT","date":"2023-09-08T02:48:00.000Z","updated":"2023-09-19T09:35:10.045Z","comments":true,"path":"2023/09/08/Vision-Transformer-ViT/","link":"","permalink":"http://zhangwenxiang.com/2023/09/08/Vision-Transformer-ViT/","excerpt":"","text":"AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE[原文]（https://arxiv.org/abs/2010.11929）中文 Vision Transformer(ViT)Paper:An Image is Worth 16x16 Words: Transformers for Image Recognition at ScaleDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … Houlsby, N. (2020). arXiv: Computer Vision and Pattern Recognition ABSTRACTWhile the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. Notes:Video:Vision Transformer (ViT) 用于图片分类 使用pytorch搭建Vision Transformer(vit)模型 Reference:[1]DOSOVITSKIY A, BEYER L, KOLESNIKOV A, et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale[J]. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition, 2020.","categories":[{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/categories/article/"}],"tags":[{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/tags/article/"},{"name":"ai","slug":"ai","permalink":"http://zhangwenxiang.com/tags/ai/"},{"name":"model","slug":"model","permalink":"http://zhangwenxiang.com/tags/model/"},{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"},{"name":"cv","slug":"cv","permalink":"http://zhangwenxiang.com/tags/cv/"},{"name":"vit","slug":"vit","permalink":"http://zhangwenxiang.com/tags/vit/"}]},{"title":"向量(vector)和张量(tensor)","slug":"向量-vector-和张量-tensor","date":"2023-09-07T03:33:57.000Z","updated":"2023-09-19T09:33:36.201Z","comments":true,"path":"2023/09/07/向量-vector-和张量-tensor/","link":"","permalink":"http://zhangwenxiang.com/2023/09/07/%E5%90%91%E9%87%8F-vector-%E5%92%8C%E5%BC%A0%E9%87%8F-tensor/","excerpt":"","text":"张量是多维数组，目的是把向量、矩阵推向更高的维度。 点——标量（scalar）线——向量（vector）面——矩阵（matrix）体——张量（tensor） 向量（vector）张量（tensor）url：PyTorch中张量的使用：http://t.csdn.cn/UdkHE PyTorchshape: 张量的形状，即各维度的大小。dtype: 张量的数据类型，例如float32、int64等。device: 张量存放的设备，例如cpu或cuda。Ref:张量（tensor）图解Vit 1：Vision Transformer——图像与Transformer基础","categories":[],"tags":[]},{"title":"Attention Is All You Need","slug":"Attention-Is-All-You-Need","date":"2023-09-06T07:38:44.000Z","updated":"2023-09-08T03:32:32.462Z","comments":true,"path":"2023/09/06/Attention-Is-All-You-Need/","link":"","permalink":"http://zhangwenxiang.com/2023/09/06/Attention-Is-All-You-Need/","excerpt":"","text":"ATTENTION IS ALL YOU NEEDPaper:Notes:Video:","categories":[{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/categories/article/"}],"tags":[{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/tags/article/"},{"name":"ai","slug":"ai","permalink":"http://zhangwenxiang.com/tags/ai/"},{"name":"nlp","slug":"nlp","permalink":"http://zhangwenxiang.com/tags/nlp/"},{"name":"model","slug":"model","permalink":"http://zhangwenxiang.com/tags/model/"},{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"}]},{"title":"NVIDIA驱动问题","slug":"NVIDIA驱动问题","date":"2023-09-06T01:59:02.000Z","updated":"2023-09-06T02:08:22.298Z","comments":true,"path":"2023/09/06/NVIDIA驱动问题/","link":"","permalink":"http://zhangwenxiang.com/2023/09/06/NVIDIA%E9%A9%B1%E5%8A%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题1：Linux服务器深度学习代码无法用GPU执行；问题2：执行命令 nvidia-smi，提示：Failed to initialize NVML: Driver&#x2F;library version mismatch原因：系统自动更新驱动分析：1.查看nvidia相关安装包信息，确认一下版本 1sudo dpkg --list | grep nvidia-* 2.查看nvidia内核版本 1cat /proc/driver/nvidia/version 3.查看安装包安装或更新情况 1cat /var/log/dpkg.log | grep nvidia 核对发现服务驱动自动更新了，所以导致显卡驱动用不了，执行不力nvidia-smi 解决办法：禁用自动更新1sudo vim /etc/apt/apt.conf.d/50unattended-upgrades 注释掉以下两行： &#x2F;&#x2F;“${distro_id}:${distro_codename}”; &#x2F;&#x2F;“${distro_id}:${distro_codename}-security”; 重启系统1sudo reboot 再次执行： 1nvidia-smi 正常： 123456789101112131415161718192021(base) dino@jarvis:~$ nvidia-smi Wed Sep 6 01:43:33 2023 +-----------------------------------------------------------------------------+| NVIDIA-SMI 525.125.06 Driver Version: 525.125.06 CUDA Version: 12.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A || 0% 57C P0 107W / 350W | 0MiB / 12288MiB | 0% Default || | | N/A |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ Ref: https://zhuanlan.zhihu.com/p/453955370","categories":[{"name":"tech","slug":"tech","permalink":"http://zhangwenxiang.com/categories/tech/"},{"name":"linux","slug":"tech/linux","permalink":"http://zhangwenxiang.com/categories/tech/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangwenxiang.com/tags/linux/"},{"name":"nvidia","slug":"nvidia","permalink":"http://zhangwenxiang.com/tags/nvidia/"},{"name":"驱动","slug":"驱动","permalink":"http://zhangwenxiang.com/tags/%E9%A9%B1%E5%8A%A8/"},{"name":"更新","slug":"更新","permalink":"http://zhangwenxiang.com/tags/%E6%9B%B4%E6%96%B0/"}]},{"title":"Linux连接SEU校园网","slug":"Linux连接SEU校园网","date":"2023-09-06T01:47:52.000Z","updated":"2023-09-06T01:59:56.659Z","comments":true,"path":"2023/09/06/Linux连接SEU校园网/","link":"","permalink":"http://zhangwenxiang.com/2023/09/06/Linux%E8%BF%9E%E6%8E%A5SEU%E6%A0%A1%E5%9B%AD%E7%BD%91/","excerpt":"","text":"SEU校园网登录页面：w.seu.edu.cn由于Linux更新显卡驱动后需要重启，导致IP重新分配。连接校园网后需要执行Web登录页面操作，Linux命令行界面如何登录，采用curl指令操作。 Linux下执行如下命令：1$ curl &quot;w.seu.edu.cn&quot; -d &quot;a=find_mac&amp;DDDDD=学号&amp;upass=密码&amp;0MKKey=&quot;","categories":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"},{"name":"tech","slug":"phd/tech","permalink":"http://zhangwenxiang.com/categories/phd/tech/"},{"name":"linux","slug":"phd/tech/linux","permalink":"http://zhangwenxiang.com/categories/phd/tech/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangwenxiang.com/tags/linux/"},{"name":"seu","slug":"seu","permalink":"http://zhangwenxiang.com/tags/seu/"},{"name":"东南大学","slug":"东南大学","permalink":"http://zhangwenxiang.com/tags/%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6/"},{"name":"校园网","slug":"校园网","permalink":"http://zhangwenxiang.com/tags/%E6%A0%A1%E5%9B%AD%E7%BD%91/"}]},{"title":"todo list 9.4","slug":"todo-list-9-4","date":"2023-09-04T13:25:28.000Z","updated":"2023-09-04T13:36:46.406Z","comments":true,"path":"2023/09/04/todo-list-9-4/","link":"","permalink":"http://zhangwenxiang.com/2023/09/04/todo-list-9-4/","excerpt":"","text":"近两日睡眠不太好，入睡太晚。上午到实验室把宣传片完成之后便进入东张西望的状态。应该是目标没有那么具体导致的注意力不集中，因为半天或者一天的任务无法达到明确的预期，所以迟迟不能进入状态。明早把手头之前的总结内容看完，列出具体的任务清单。将任务分解后逐步完成。 接下来的具体任务：Classification by ViT","categories":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"}],"tags":[{"name":"todo-list","slug":"todo-list","permalink":"http://zhangwenxiang.com/tags/todo-list/"}]},{"title":"宣传片","slug":"宣传片","date":"2023-09-04T02:09:30.000Z","updated":"2023-09-04T02:12:38.631Z","comments":true,"path":"2023/09/04/宣传片/","link":"","permalink":"http://zhangwenxiang.com/2023/09/04/%E5%AE%A3%E4%BC%A0%E7%89%87/","excerpt":"","text":"西藏自治区网络安全和信息化展览从收到素材大概一天半时间，我的周末啊！","categories":[{"name":"work","slug":"work","permalink":"http://zhangwenxiang.com/categories/work/"}],"tags":[{"name":"tibet","slug":"tibet","permalink":"http://zhangwenxiang.com/tags/tibet/"},{"name":"宣传片","slug":"宣传片","permalink":"http://zhangwenxiang.com/tags/%E5%AE%A3%E4%BC%A0%E7%89%87/"}]},{"title":"A New Start","slug":"A-New-Start","date":"2023-09-01T07:01:24.000Z","updated":"2023-09-01T07:40:29.277Z","comments":true,"path":"2023/09/01/A-New-Start/","link":"","permalink":"http://zhangwenxiang.com/2023/09/01/A-New-Start/","excerpt":"","text":"重新开始生涯记录，在SEU博士第三年伊始。 问题学习阅读不连续外界干扰很多，有工作的有生活的也有杂七杂八的事情。 每日总结每天结束时记录工作情况，思考并总结。","categories":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"}],"tags":[{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"},{"name":"diary","slug":"diary","permalink":"http://zhangwenxiang.com/tags/diary/"},{"name":"notes","slug":"notes","permalink":"http://zhangwenxiang.com/tags/notes/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-08-29T02:55:29.224Z","updated":"2023-08-29T02:55:29.224Z","comments":true,"path":"2023/08/29/hello-world/","link":"","permalink":"http://zhangwenxiang.com/2023/08/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"questions","slug":"questions","permalink":"http://zhangwenxiang.com/categories/questions/"},{"name":"toturial","slug":"questions/toturial","permalink":"http://zhangwenxiang.com/categories/questions/toturial/"},{"name":"tutorial","slug":"tutorial","permalink":"http://zhangwenxiang.com/categories/tutorial/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/categories/article/"},{"name":"tech","slug":"tech","permalink":"http://zhangwenxiang.com/categories/tech/"},{"name":"linux","slug":"tech/linux","permalink":"http://zhangwenxiang.com/categories/tech/linux/"},{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/categories/phd/"},{"name":"tech","slug":"phd/tech","permalink":"http://zhangwenxiang.com/categories/phd/tech/"},{"name":"linux","slug":"phd/tech/linux","permalink":"http://zhangwenxiang.com/categories/phd/tech/linux/"},{"name":"work","slug":"work","permalink":"http://zhangwenxiang.com/categories/work/"}],"tags":[{"name":"cv","slug":"cv","permalink":"http://zhangwenxiang.com/tags/cv/"},{"name":"transformer","slug":"transformer","permalink":"http://zhangwenxiang.com/tags/transformer/"},{"name":"vit","slug":"vit","permalink":"http://zhangwenxiang.com/tags/vit/"},{"name":"timm","slug":"timm","permalink":"http://zhangwenxiang.com/tags/timm/"},{"name":"pytorch","slug":"pytorch","permalink":"http://zhangwenxiang.com/tags/pytorch/"},{"name":"article","slug":"article","permalink":"http://zhangwenxiang.com/tags/article/"},{"name":"ai","slug":"ai","permalink":"http://zhangwenxiang.com/tags/ai/"},{"name":"model","slug":"model","permalink":"http://zhangwenxiang.com/tags/model/"},{"name":"phd","slug":"phd","permalink":"http://zhangwenxiang.com/tags/phd/"},{"name":"nlp","slug":"nlp","permalink":"http://zhangwenxiang.com/tags/nlp/"},{"name":"linux","slug":"linux","permalink":"http://zhangwenxiang.com/tags/linux/"},{"name":"nvidia","slug":"nvidia","permalink":"http://zhangwenxiang.com/tags/nvidia/"},{"name":"驱动","slug":"驱动","permalink":"http://zhangwenxiang.com/tags/%E9%A9%B1%E5%8A%A8/"},{"name":"更新","slug":"更新","permalink":"http://zhangwenxiang.com/tags/%E6%9B%B4%E6%96%B0/"},{"name":"seu","slug":"seu","permalink":"http://zhangwenxiang.com/tags/seu/"},{"name":"东南大学","slug":"东南大学","permalink":"http://zhangwenxiang.com/tags/%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6/"},{"name":"校园网","slug":"校园网","permalink":"http://zhangwenxiang.com/tags/%E6%A0%A1%E5%9B%AD%E7%BD%91/"},{"name":"todo-list","slug":"todo-list","permalink":"http://zhangwenxiang.com/tags/todo-list/"},{"name":"tibet","slug":"tibet","permalink":"http://zhangwenxiang.com/tags/tibet/"},{"name":"宣传片","slug":"宣传片","permalink":"http://zhangwenxiang.com/tags/%E5%AE%A3%E4%BC%A0%E7%89%87/"},{"name":"diary","slug":"diary","permalink":"http://zhangwenxiang.com/tags/diary/"},{"name":"notes","slug":"notes","permalink":"http://zhangwenxiang.com/tags/notes/"}]}